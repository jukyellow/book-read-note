# 자연어 처리 딥러닝 캠프


### 2장. 기초수학


### 4장. 전처리
#### 4-5)분절: 쉼표, 인용표등 띄어쓰기 추가
#### 4-7)서브워드 분절: 조합된 단어를 다시 세분화하여 분리
#### 4-8)분절복원
#### 4-9)토치텍스트: text처리에 필요한 다양한 library이미 존재

### 5장. 유사성과 모호성
- 특징벡터 추출(data mining?)
- 유사성과 모호성: 복수표현, 조사에 따른 문법변화 등등 자연어 처리의 난관 존재?

### 6장. 단어 임베딩
- 흔한 오해1: pre-trained vec는 특정 도메인에 최적화 된 모델이 아니므로, 효과가 낮을수 있다  
   (기본 모델-> 성능개선 방안의 하나로 pre-trained를 활용하는걸 추천)  
#### 6.5 Glove: 단어의 사전적 확률 분포로 판단
#### 6.6 FastText 예

### 7장. 시퀀스 모델링
#### 7.3 LSTM => 케라스에서 Dense(fully connected)말고 LSTM이나 GRU로 돌려보기
#### 7.4 GRU
#### 7.5 Gradient 클리핑

### 8장. 텍스트 분류
- RNN 예제

### 9장. 언어모델링
#### 9.2 n-gram: n-gram방식으로 임베딩 vec 직접 구현?
#### 9.3 NNLM: n-gram방식의 언어모델 약점보완(신규 단어(희소성)에 효과적? FastText쓰면안돼?)
 -RNNLM-> (RNN)->(LSTM)으로 변경한 모델 예제

### 10.장 신경망 기계번역
#### 10.2 seq2seq: 문장 생성 모델, 한계 - 긴 문장에서 성능 떨어짐
#### 10.3 Attention: encoder+decoder 모델에서 decoder가 encoder의 vec를 참조하여 최종 결과를 만들고, softmax로 전달
#### 10.4 input feeding : 어텐션+input feeding으로 성능 추가 향상

### 11.기계번역 심화주제
- 트랜스포머(BERT 기)

### 12.장 강화학습을 활용한 자연어 생성

### 15장. 전이학습
#### 15.1 전이학습이란?
#### 15.4 BERT

